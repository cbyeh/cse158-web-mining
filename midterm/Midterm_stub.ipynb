{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy import spatial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(f):\n",
    "    for l in gzip.open(f):\n",
    "        yield eval(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data from below:\n",
    "# https://drive.google.com/uc?id=1V4MLeoEiPQdocCbUHjR_7L9ZmxTufPFe\n",
    "# Or:\n",
    "# https://cseweb.ucsd.edu/classes/fa20/cse258-a/files/\n",
    "dataset = list(parse(\"goodreads_reviews_comics_graphic.json.gz\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "542338"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'bafc2d50014200cda7cb2b6acd60cd73',\n",
       " 'book_id': '6315584',\n",
       " 'review_id': '72f1229aba5a88f9e72f0dcdc007dd22',\n",
       " 'rating': 4,\n",
       " 'review_text': \"I've never really liked Spider-Man. I am, however, a huge fan of the Dresden Files. Jim Butcher is clever and sarcastic and probably the perfect choice to pen a superhero novel. I really enjoyed this book!\",\n",
       " 'date_added': 'Wed Aug 10 06:06:48 -0700 2016',\n",
       " 'date_updated': 'Fri Aug 12 08:49:54 -0700 2016',\n",
       " 'read_at': 'Fri Aug 12 08:49:54 -0700 2016',\n",
       " 'started_at': 'Wed Aug 10 00:00:00 -0700 2016',\n",
       " 'n_votes': 0,\n",
       " 'n_comments': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature(d):\n",
    "    dayFeat = [0]*7 # One hot encoding of day of week\n",
    "    dayDict = {\"Mon\":0, \"Tue\":1, \"Wed\":2, \"Thu\":3, \"Fri\":4, \"Sat\":5, \"Sun\":6}\n",
    "    dayFeat[dayDict[d['date_added'][:3]]] = 1\n",
    "    return [1, d['rating'], d['n_comments']] + dayFeat[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [feature(d) for d in dataset]\n",
    "y = [len(d['review_text']) for d in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "yPred = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = MSE(yPred, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624989.9720071985"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1a / 2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93882.12153472501"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A disadvantage would be overfitting, and may not represent an overall good predictor,\n",
    "# as it would leave out many data points that are outliers. We also lose many data points resulting in smaller training,\n",
    "# and would not be good for predicting data with large review lengths\n",
    "\n",
    "# An advantage would be that it reduces the MSE overall as our model is less dynamic\n",
    "# It also decreases the time to train as we have fewer data points\n",
    "\n",
    "\n",
    "# Choose the kept values based on Interquartile range, and build new X and y within that range. An interquartile range\n",
    "# selects within a certain percentile, we can use this to discard values too low, and too high. I chose this because it is a\n",
    "# reasonable range, representing a bell curve, and removing the majority of outliers\n",
    "high, low = np.percentile(y, [75, 25]) # 247, 94, chosen as the 1st and 3rd quarter labels, because outliers are too high or low\n",
    "iqr = high - low\n",
    "X_outlier = []\n",
    "y_outlier = []\n",
    "\n",
    "# Remove outliers\n",
    "for i in range(len(y)):\n",
    "    if (not y[i] <= low - iqr * 1.5 and not y[i] >= high + iqr * 1.5):\n",
    "        X_outlier.append(X[i])\n",
    "        y_outlier.append(y[i])\n",
    "\n",
    "# Predict\n",
    "model = sklearn.linear_model.LinearRegression()     \n",
    "model.fit(X_outlier, y_outlier)\n",
    "yPred = model.predict(X_outlier)\n",
    "mse = MSE(yPred, y_outlier)\n",
    "\n",
    "# As we can see, the MSE is better as it predicts non-outliers much better, which may show an improved predictor. However\n",
    "# we are dealing with less information so we cannot say much for variable review lengths\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1b/2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158.4185033539574, 0.0015632564605444928)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A disadvantage would be that the significance of the variance is lost, i.e. super long reviews are treated as much shorter\n",
    "\n",
    "# An advantage would be a better representation of length of reviews, as they are less dynamic\n",
    "# and better controlled. The curve of the model is also flattened to better fit a straight line, with much better MSE\n",
    "\n",
    "\n",
    "# Apply square root to length of reviews\n",
    "# I chose this as it is a good way to transform y to reduced values, similar to log\n",
    "y_sqrt = [math.sqrt(len(d['review_text'])) for d in dataset]\n",
    "\n",
    "# Normalize length of reviews\n",
    "# I chose this as it is an intuitive way to reduce y\n",
    "max = np.max(y)\n",
    "y_normalize = [len(d['review_text']) / max for d in dataset]\n",
    "\n",
    "\n",
    "# Predict\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X, y_sqrt)\n",
    "yPred = model.predict(X)\n",
    "mse_sqrt = MSE(yPred, y_sqrt)\n",
    "\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X, y_normalize)\n",
    "yPred = model.predict(X)\n",
    "mse_normalize = MSE(yPred, y_normalize)\n",
    "\n",
    "# Overall I would choose the normalized predictor as the MSE is better. The performances were vastly improved to the original\n",
    "# based on our MSE, but that may have to due with our y having smaller values as well\n",
    "mse_sqrt, mse_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1c/2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple classifier, that just decides whether it is above or below median. I chose this because it is deterministic\n",
    "def binary_outcome(median, val):\n",
    "    if val <= median:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4989102736669752"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disadvantages include getting less information out of predicting any dataset, since we can only get two possibilities,\n",
    "# instead of a value\n",
    "# It also cannot estimate a review length beyond whether it is below or above a value\n",
    "\n",
    "# An advantage would be more clear in a prediction with a true or false, rather than an obscure value\n",
    "\n",
    "\n",
    "# Predict accuracy\n",
    "median = np.median(y)\n",
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X, y)\n",
    "yPred = model.predict(X)\n",
    "correct = 0\n",
    "for i in range(len(y)):\n",
    "    if binary_outcome(median, y[i]) == binary_outcome(median, yPred[i]):\n",
    "        correct += 1\n",
    "accuracy = correct / len(y)\n",
    "\n",
    "# Accuracy is 50% so this predictor is trivial and does not perform well. This may be due to the many outliers\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 1d/2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose this objective as it is less sensitive to outliers. Compared to MSE, the difference is not squared\n",
    "def MAE(predictions, labels):\n",
    "    differences = [abs((x-y)) for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "464.48566706945536"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Disadvantages include it being less effective with a tight dataset\n",
    "# It fails to punish large errors\n",
    "\n",
    "# Advantages include it being less sensitive to outliers, as error differences are not exponentiated\n",
    "\n",
    "\n",
    "# Predict\n",
    "model = sklearn.linear_model.LinearRegression()     \n",
    "model.fit(X, y)\n",
    "yPred = model.predict(X)\n",
    "mae = MAE(yPred, y)\n",
    "\n",
    "# Our value is smaller than the MSE as it is less sensitive to outliers. This reduces our error number, but this approach\n",
    "# might not be good for tighter datasets\n",
    "mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 3\n",
    "\n",
    "Note: You can insert an image (e.g. containing a hand-written solution) via edit->insert image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) \n",
    "Good features to estimate tips of a person taking a taxicab trip may be the following:\n",
    "    \n",
    "    * religion: some faiths believe in being generous\n",
    "    * ethnicity: race of driver\n",
    "    * salary: a higher salary means more to give, or a lower salary may make someone more generous\n",
    "    \n",
    "Good features of a trip itself may be:\n",
    "    \n",
    "    * time_taken: how long to complete a trip\n",
    "    * number_of_tolls: how many toll roads were taken\n",
    "    * amount_of_passengers: how many passengers were present\n",
    "    * weather: whether it was raining, snowing, etc\n",
    "    * geography: urban, rural, etc\n",
    "    * ethnicity_match: if driver and passengers were all the same ethnicity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b)\n",
    "    * religion: can be one-hot-encoded for a finite amount of religions\n",
    "    * ethnicity: race of driver\n",
    "    * salary: can be represented as an integer\n",
    "    \n",
    "    * time_taken: can be represented as an integer of the number of minutes\n",
    "    * number_of_tolls: can be represented as an integer\n",
    "    * amount_of_passengers: can be represented as an integer\n",
    "    * weather: can be categorically encoded. i.e. 0 for sunny, 1 for raining, ...\n",
    "    * geography: can be categorically encoded. i.e. 0 for urban, 1 for rural, ...\n",
    "    * ethnicity_match: can be a boolean. 1 if they're all the same ethnicty, 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c)\n",
    "This would best be represented as a regression problem. The goal would be to predict the tip that would be given by the passenger(s). Transforming with reduction or removal might be useful for salary might be useful as there may be people who make 0 or too much money e.g. a billionaire, however it shouldn't be a problem for most use cases as salaries usually range to a fixed amount. Transforming other features may be useful as well, e.g. if there are only Buddhists who ride a taxi in a dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 (Recommender Systems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code based on http://cseweb.ucsd.edu/classes/fa20/cse258-a/code/workbook4.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility data structures\n",
    "reviewsPerUser = defaultdict(list)\n",
    "reviewsPerItem = defaultdict(list)\n",
    "usersPerItem = defaultdict(set) # U_i from class slides\n",
    "itemsPerUser = defaultdict(set) # I_u from class slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dataset:\n",
    "    user,item = d['user_id'], d['book_id']\n",
    "    reviewsPerUser[user].append(d)\n",
    "    reviewsPerItem[item].append(d)\n",
    "    usersPerItem[item].add(user)\n",
    "    itemsPerUser[user].add(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingMean = sum([d['rating'] for d in dataset]) / len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.778138356523054"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Jaccard(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = len(s1.union(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should be re-defined for each of your model variants\n",
    "def predictRating(user, item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 'bafc2d50014200cda7cb2b6acd60cd73',\n",
       " 'book_id': '6315584',\n",
       " 'review_id': '72f1229aba5a88f9e72f0dcdc007dd22',\n",
       " 'rating': 4,\n",
       " 'review_text': \"I've never really liked Spider-Man. I am, however, a huge fan of the Dresden Files. Jim Butcher is clever and sarcastic and probably the perfect choice to pen a superhero novel. I really enjoyed this book!\",\n",
       " 'date_added': 'Wed Aug 10 06:06:48 -0700 2016',\n",
       " 'date_updated': 'Fri Aug 12 08:49:54 -0700 2016',\n",
       " 'read_at': 'Fri Aug 12 08:49:54 -0700 2016',\n",
       " 'started_at': 'Wed Aug 10 00:00:00 -0700 2016',\n",
       " 'n_votes': 0,\n",
       " 'n_comments': 0}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.44493246042927"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u,i = dataset[1]['user_id'], dataset[1]['book_id']\n",
    "predictRating(u,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(dataset, 1000)\n",
    "sampleLabels = [d['rating'] for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline prediction\n",
    "alwaysPredictMean = [ratingMean for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using item-to-item similarity above\n",
    "cfPredictions = [predictRating(d['user_id'], d['book_id']) for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3390861315078861"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline accuracy\n",
    "MSE(alwaysPredictMean, sampleLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0296334594874024"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Item-to-item similarity accuracy\n",
    "MSE(cfPredictions, sampleLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 (a) (i.e., the first of your three variants)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interchange users and items from the previous function\n",
    "def predictRating(user, item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerItem[item]:\n",
    "        u2 = d['user_id']\n",
    "        if u2 == user: continue\n",
    "        ratings.append(d['rating'])\n",
    "        similarities.append(Jaccard(itemsPerUser[user],itemsPerUser[u2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using user-to-user similarity above\n",
    "cfPredictions = [predictRating(d['user_id'], d['book_id']) for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3970801064452454"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Item-to-item similarity accuracy\n",
    "MSE(cfPredictions, sampleLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose overlap coefficient instead of Jaccard. This divides the numerator by the min of s1 or s2\n",
    "def Overlap(s1, s2):\n",
    "    numer = len(s1.intersection(s2))\n",
    "    denom = min(len(s1), len(s2))\n",
    "    return numer / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use original predict rating but with Cosine\n",
    "def predictRating(user, item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    for d in reviewsPerItem[item]:\n",
    "        u2 = d['user_id']\n",
    "        if u2 == user: continue\n",
    "        ratings.append(d['rating'])\n",
    "        similarities.append(Overlap(itemsPerUser[user],itemsPerUser[u2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
    "        return sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using item-to-item similarity above\n",
    "cfPredictions = [predictRating(d['user_id'], d['book_id']) for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3768558121005727"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Item-to-item similarity accuracy\n",
    "MSE(cfPredictions, sampleLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 7 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now initially subtract the mean rating, so that we are weighting deviations from the mean\n",
    "def predictRating(user, item):\n",
    "    ratings = []\n",
    "    similarities = []\n",
    "    averages = []\n",
    "    for d in reviewsPerUser[user]:\n",
    "        i2 = d['book_id']\n",
    "        if i2 == item: continue\n",
    "        ratings.append(d['rating'])\n",
    "        similarities.append(Jaccard(usersPerItem[item],usersPerItem[i2]))\n",
    "        averages.append(sum(d['rating'] for d in reviewsPerItem[i2]) / len(reviewsPerItem[i2]))\n",
    "    if (sum(similarities) > 0):\n",
    "        weightedRatings = [((x - mean)*y) for x,y,mean in zip(ratings,similarities,averages)]\n",
    "        avg_item = sum([d['rating'] for d in reviewsPerItem[item]]) / len(reviewsPerItem[item])\n",
    "        return avg_item + sum(weightedRatings) / sum(similarities)\n",
    "    else:\n",
    "        # User hasn't rated any similar items\n",
    "        return ratingMean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction using item-to-item similarity above and mean\n",
    "cfPredictions = [predictRating(d['user_id'], d['book_id']) for d in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8225498367471318"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Item-to-item similarity accuracy\n",
    "MSE(cfPredictions, sampleLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
